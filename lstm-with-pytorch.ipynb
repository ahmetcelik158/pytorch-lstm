{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS April 2022 - Time Series Classification\n\nIn Kaggle's TPS April 2022 competition, we are challenged a time series classification problem. The dataset contains biological sensor data recorded from different participants. Each observation is a sixty second recordings from 13 sensors which has a state as either 0 or 1. While the train set has nearly 26.000 sequences, we will be classifying nearly 12.000 sequence of test set.\n\nI am going to implement RNN with LSTM layers for this problem. This will be my first experience with RNN. There were great kernels that helped me to understand LSTM, i mentioned them in references \\[1-4\\]. The structure of the notebook will be as follows:\n\n**Index**\n\n1. [Data Preparation](#1.-Data-Preparation)\n\n2. [Model Definition](#2.-Model-Definition)\n\n3. [Function Definitions - Validation, Training and Prediction](#3.-Function-Definitions---Validation,-Training-and-Prediction)\n\n4. [Training the Model](#4.-Training-the-Model)\n\n5. [Prediction](#5.-Prediction)\n\n6. [References](#6.-References)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntrain = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/test.csv\")\ntrain_labels = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv\")\n\ntrain = train.set_index([\"sequence\", \"subject\", \"step\"])\ntest = test.set_index([\"sequence\", \"subject\", \"step\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Preparation\n\nIn this section, I will\n* check missing values\n* add new features\n* scale data\n* reshape data\n* create tensors and dataloaders\n\n\nI created lag, difference, rolling mean and rolling std for new features. While plotting some sensors in time domain, I saw instant value drops at the last second (for example sequence=0, sensor=12). I thought that these sudden drops may be noises while closing the sensor and I should not trust beginning and ending of the sensor data. By dropping NaN values after using a centered window with size 5 in add_features function, I have automatically dropped the first and last 2 seconds. As a result, I will be using 56 seconds of data to train my model.","metadata":{}},{"cell_type":"code","source":"print(\"Checking if there are any missing values:\")\nprint(\"Train: {}\".format(train.isnull().sum().sum()))\nprint(\"Test: {}\".format(test.isnull().sum().sum()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df, features):\n    for feature in features:\n        df_grouped = df.groupby(\"sequence\")[feature]\n        df_rolling = df_grouped.rolling(5, center=True)\n        \n        df[feature + \"_lag1\"] = df_grouped.shift(1)\n        df[feature + \"_diff1\"] = df[feature] - df[feature + \"_lag1\"]\n        df[feature + \"_lag2\"] = df_grouped.shift(2)\n        df[feature + \"_diff2\"] = df[feature] - df[feature + \"_lag2\"]\n        df[feature + \"_roll_mean\"] = df_rolling.mean().reset_index(0, drop=True)\n        df[feature + \"_roll_std\"] = df_rolling.std().reset_index(0, drop=True)\n    df.dropna(axis=0, inplace=True)\n    return\n\nfeatures = [\"sensor_{:02d}\".format(i) for i in range(13)]\nadd_features(train, features)\nadd_features(test, features)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = train.shape[1]\nsequence_length = len(train.index.get_level_values(2).unique())\n\n# Scaling test and train\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n\n# Reshaping:\ntrain = train.reshape(-1, sequence_length, input_size)\ntest = test.reshape(-1, sequence_length, input_size)\nprint(\"After Reshape\")\nprint(\"Shape of training set: {}\".format(train.shape))\nprint(\"Shape of test set: {}\".format(test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting train data set into train and validation sets\n# validation size is selected as 0.2\nt_X, v_X, t_y, v_y = train_test_split(train, train_labels.state, test_size=0.20,\n                                      shuffle=True, random_state=0)\n\n# Converting train, validation and test data into tensors\ntrain_X_tensor = torch.tensor(t_X).float()\nval_X_tensor = torch.tensor(v_X).float()\ntest_tensor = torch.tensor(test).float()\n\n# Converting train and validation labels into tensors\ntrain_y_tensor = torch.tensor(t_y.values)\nval_y_tensor = torch.tensor(v_y.values)\n\n# Creating train and validation tensors\ntrain_tensor = TensorDataset(train_X_tensor, train_y_tensor)\nval_tensor = TensorDataset(val_X_tensor, val_y_tensor)\n\n# Defining the dataloaders\ndataloaders = dict()\ndataloaders[\"train\"] = DataLoader(train_tensor, batch_size=64, shuffle=True)\ndataloaders[\"val\"] = DataLoader(val_tensor, batch_size=32)\ndataloaders[\"test\"] = DataLoader(test_tensor, batch_size=32)\nprint(\"Dataloaders are created!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model Definition\n\nIn this section, I will define my RNN model including LSTM layers. For LSTM layers, input and output shapes will be as follows:\n\n* Input: (batch_size, sequence_length, input_size)\n* Output: (batch_size, sequence_length, D * hidden_size)\n\nwhere D is 2 for bidirectional LSTM, otherwise 1. For details, check references for PyTorch LSTM documentation \\[5\\].","metadata":{}},{"cell_type":"code","source":"# Definition of a RNN Model class\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_sizes, seq_len, dropout=0.5, output_size=1):\n        super(RNN, self).__init__()\n        \n        # LSTM Layers\n        self.lstm_1 = nn.LSTM(input_size, hidden_sizes[0], num_layers=2,\n                            batch_first=True, bidirectional=True, dropout=dropout)\n        self.lstm_21 = nn.LSTM(2*hidden_sizes[0], hidden_sizes[1], num_layers=2,\n                             batch_first=True, bidirectional=True, dropout=dropout)\n        self.lstm_22 = nn.LSTM(input_size, hidden_sizes[1], num_layers=2,\n                             batch_first=True, bidirectional=True, dropout=dropout)\n        self.lstm_31 = nn.LSTM(2*hidden_sizes[1], hidden_sizes[2], num_layers=2,\n                             batch_first=True, bidirectional=True, dropout=dropout)\n        self.lstm_32 = nn.LSTM(4*hidden_sizes[1], hidden_sizes[2], num_layers=2,\n                             batch_first=True, bidirectional=True, dropout=dropout)\n        self.lstm_41 = nn.LSTM(2*hidden_sizes[2], hidden_sizes[3], num_layers=2,\n                             batch_first=True, bidirectional=True, dropout=dropout)\n        self.lstm_42 = nn.LSTM(4*hidden_sizes[2], hidden_sizes[3], num_layers=2,\n                             batch_first=True, bidirectional=True, dropout=dropout)\n        hidd = 2*hidden_sizes[0] + 4*(hidden_sizes[1]+hidden_sizes[2]+hidden_sizes[3])\n        self.lstm_5 = nn.LSTM(hidd, hidden_sizes[4], num_layers=2,\n                             batch_first=True, bidirectional=True, dropout=dropout)\n        \n        # Fully Connected Layer\n        self.fc = nn.Sequential(nn.Linear(2*hidden_sizes[4]*seq_len, 4096),\n                                nn.ReLU(inplace=True),\n                                nn.Dropout(p=dropout),\n                                nn.Linear(4096, 1024),\n                                nn.ReLU(inplace=True),\n                                nn.Dropout(p=dropout),\n                                nn.Linear(1024, output_size),\n                                nn.Sigmoid()\n                               )\n        \n    def forward(self, x):\n        # lstm layers:\n        x1, _ = self.lstm_1(x)\n        \n        x_x1, _ = self.lstm_21(x1)\n        x_x2, _ = self.lstm_22(x)\n        x2 = torch.cat([x_x1, x_x2], dim=2)\n        \n        x_x1, _ = self.lstm_31(x_x1)\n        x_x2, _ = self.lstm_32(x2)\n        x3 = torch.cat([x_x1, x_x2], dim=2)\n        \n        x_x1, _ = self.lstm_41(x_x1)\n        x_x2, _ = self.lstm_42(x3)\n        x4 = torch.cat([x_x1, x_x2], dim=2)\n        x = torch.cat([x1, x2, x3, x4], dim=2)\n        x, _ = self.lstm_5(x)\n        \n        # fully connected layers:\n        x = x.reshape(x.shape[0], -1)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Function Definitions - Validation, Training and Prediction","metadata":{}},{"cell_type":"code","source":"### VALIDATION FUNCTION\ndef validation(model, loader, criterion, device=\"cpu\"):\n    model.eval()\n    loss = 0\n    preds_all = torch.LongTensor()\n    labels_all = torch.LongTensor()\n    \n    with torch.no_grad():\n        for batch_x, labels in loader:\n            labels_all = torch.cat((labels_all, labels), dim=0)\n            batch_x, labels = batch_x.to(device), labels.to(device)\n            labels = labels.unsqueeze(1).float()\n            \n            output = model.forward(batch_x)\n            loss += criterion(output,labels).item()\n            preds_all = torch.cat((preds_all, output.to(\"cpu\")), dim=0)\n    total_loss = loss/len(loader)\n    auc_score = roc_auc_score(labels_all, preds_all)\n    return total_loss, auc_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### TRAINING FUNCTION\ndef train_model(model, trainloader, validloader, criterion, optimizer, \n                scheduler, epochs=20, device=\"cpu\", print_every=1):\n    model.to(device)\n    best_auc = 0\n    best_epoch = 0\n    for e in range(epochs):\n        model.train()\n        \n        for batch_x, labels in trainloader:\n            batch_x, labels = batch_x.to(device), labels.to(device)\n            labels = labels.unsqueeze(1).float()\n            \n            # Training \n            optimizer.zero_grad()\n            output = model.forward(batch_x)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n        # at the end of each epoch calculate loss and auc score:\n        model.eval()\n        train_loss, train_auc = validation(model, trainloader, criterion, device)\n        valid_loss, valid_auc = validation(model, validloader, criterion, device)\n        if valid_auc > best_auc:\n            best_auc = valid_auc\n            best_epoch = e\n            torch.save(model.state_dict(), \"best-state.pt\")\n        if e % print_every == 0:\n            to_print = \"Epoch: \"+str(e+1)+\" of \"+str(epochs)\n            to_print += \".. Train Loss: {:.4f}\".format(train_loss)\n            to_print += \".. Valid Loss: {:.4f}\".format(valid_loss)\n            to_print += \".. Valid AUC: {:.3f}\".format(valid_auc)\n            print(to_print)\n    # After Training:\n    model.load_state_dict(torch.load(\"best-state.pt\"))\n    to_print = \"\\nTraining completed. Best state dict is loaded.\\n\"\n    to_print += \"Best Valid AUC is: {:.4f} after {} epochs\".format(best_auc,best_epoch+1)\n    print(to_print)\n    return","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### PREDICTION FUNCTION\ndef prediction(model, loader, device=\"cpu\"):\n    model.to(device)\n    model.eval()\n    preds_all = torch.LongTensor()\n    \n    with torch.no_grad():\n        for batch_x in loader:\n            batch_x = batch_x.to(device)\n            \n            output = model.forward(batch_x).to(\"cpu\")\n            preds_all = torch.cat((preds_all, output), dim=0)\n    return preds_all","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training the Model\n\nIn this section, I will\n* initiate RNN model\n* define criterion, optimizer and schedular\n* train the model\n\nI selected OneCycleLR as a schedular to adjust learning rate. With OneCycleLR, learning rate will increase at the beginning until max learning rate, and then start decreasing. I used pct_start = 0.2, so learning rate will reach to max value at the 0.2 of the learning phase. You can check references \\[6,7\\] for more details.","metadata":{}},{"cell_type":"code","source":"hidden_sizes = [288, 192, 144, 96, 32]\nmax_learning_rate = 0.001\nepochs = 41\n\n# Model\nmodel_lstm = RNN(input_size, hidden_sizes, sequence_length)\nprint(\"Model: \")\nprint(model_lstm)\n\n# criterion, optimizer, scheduler\n#criterion = nn.MSELoss()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model_lstm.parameters(), lr=max_learning_rate)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer,\n                                          max_lr = max_learning_rate,\n                                          epochs = epochs,\n                                          steps_per_epoch = len(dataloaders[\"train\"]),\n                                          pct_start = 0.2,\n                                          anneal_strategy = \"cos\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking if GPU is available\nif torch.cuda.is_available():\n    my_device = \"cuda\"\n    print(\"GPU is enabled\")\nelse:\n    my_device = \"cpu\"\n    print(\"No GPU :(\")\n\n# Training\ntrain_model(model = model_lstm,\n            trainloader = dataloaders[\"train\"],\n            validloader = dataloaders[\"val\"],\n            criterion = criterion,\n            optimizer = optimizer,\n            scheduler = scheduler,\n            epochs = epochs,\n            device = my_device,\n            print_every = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Prediction\n\nIn this section, I will\n* predict the test dataset\n* submit my results","metadata":{}},{"cell_type":"code","source":"y_pred = prediction(model_lstm, dataloaders[\"test\"], device=my_device)\nprint(\"Prediction completed, first 5 states:\")\ny_pred[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/tabular-playground-series-apr-2022/sample_submission.csv\")\nsubmission.state = y_pred\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nprint(\"Resuls are saved to submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. References\n\n1. [Tps April Tensorflow Bi-LSTM by @hamzaghanmi](https://www.kaggle.com/code/hamzaghanmi/tps-april-tensorflow-bi-lstm)\n2. [TPSApr22 - FE + Pseudo Labels + Bi-LSTM by @hasanbasriakcay](https://www.kaggle.com/code/hasanbasriakcay/tpsapr22-fe-pseudo-labels-bi-lstm)\n3. [[TPS-Apr][PyTorch] Bidirectional LSTM by @lordozvlad](https://www.kaggle.com/code/lordozvlad/tps-apr-pytorch-bidirectional-lstm)\n4. [TPS Apr22 - EDA / FE + LSTM Tutorial by @javigallego](https://www.kaggle.com/code/javigallego/tps-apr22-eda-fe-lstm-tutorial)\n5. [PyTorch LSTM documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n6. [PyTorch documentation for adjusting learning rate](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n7. [Guide to Pytorch Learning Rate Scheduling by @isbhargav](https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling/notebook)","metadata":{}}]}